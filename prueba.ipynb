{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ignacio.milesi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ignacio.milesi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ignacio.milesi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\ignacio.milesi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ignacio.milesi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ignacio.milesi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "import joblib\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funciones para el procesamiento del texto\n",
    "stop_words = set(stopwords.words('english'))\n",
    "count_vectorizer = CountVectorizer\n",
    "\n",
    "def vectorize_text(text: list[str]):\n",
    "    \"\"\"This function transforms data for prediction\"\"\"\n",
    "    X_vectorized = count_vectorizer.transform([text])\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    tfidf_matrix = tfidf_transformer.fit_transform(X_vectorized)\n",
    "    return tfidf_matrix\n",
    "\n",
    "def preprocessing_fn(text):\n",
    "    # tokenization, removing stopwords\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens_without_stopwords = [word for word in tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    # lemmatization, and extracting nouns\n",
    "    tagged = pos_tag(tokens_without_stopwords)\n",
    "    array_nouns = [word for word, pos in tagged if pos.startswith('NN')]\n",
    "    nouns = ' '.join(array_nouns)\n",
    "\n",
    "    \n",
    "    X_vectorized = vectorize_text(nouns)\n",
    "    return X_vectorized\n",
    "\n",
    "\n",
    "def run_preprocessing_fn(X):\n",
    "    \"\"\" This functions runs the preprocessing pipeline \"\"\"\n",
    "    processed_data = [preprocessing_fn(text) for text in X]\n",
    "\n",
    "    data_vectorizada= [sparse_matrix.toarray() for sparse_matrix in processed_data]\n",
    "    data_vectorizada = np.vstack(data_vectorizada) \n",
    "    \n",
    "    return data_vectorizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(path: str, file_name: str):\n",
    "        \"\"\"This method is used to read the json file\"\"\"\n",
    "        file_path = os.path.join(path, file_name)\n",
    "        with open(file_path, encoding=\"utf8\") as file:\n",
    "            datos = json.load(file)\n",
    "        df_tickets = pd.json_normalize(datos)\n",
    "        return df_tickets\n",
    "\n",
    "data_sin_procesar = read_json(\"data/data_sin_procesar\", \"tickets_classification_eng.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_source.complaint_what_happened</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4654</th>\n",
       "      <td>On XX/XX/2020, I called Chase Sapphire reserve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70771</th>\n",
       "      <td>On XXXX/XXXX/XXXX I was approve for a trial mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29982</th>\n",
       "      <td>I am facing foreclosure after I have been tryi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         _source.complaint_what_happened\n",
       "4654   On XX/XX/2020, I called Chase Sapphire reserve...\n",
       "70771  On XXXX/XXXX/XXXX I was approve for a trial mo...\n",
       "29982  I am facing foreclosure after I have been tryi..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sin_procesar_corta = data_sin_procesar[[\"_source.complaint_what_happened\"]]\n",
    "\n",
    "data_analizar = data_sin_procesar_corta[data_sin_procesar_corta[\"_source.complaint_what_happened\"]!= \"\"].sample(20)\n",
    "\n",
    "data_analizar.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\n",
    "    \"0\": \"Bank Account Services\",\n",
    "    \"1\": \"Credit Report or Prepaid Card\",\n",
    "    \"2\": \"Mortgage/Loan\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Disco d\\Milesi\\01 - Personal\\platzi\\07 - Curso de MLOPS\\ent-deploy-serving\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 1.4.0 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = joblib.load(\"app/count_vectorizer.pkl\")\n",
    "\n",
    "model = joblib.load(\"model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Credit Report or Prepaid Card', 'Mortgage/Loan', 'Mortgage/Loan', 'Mortgage/Loan', 'Credit Report or Prepaid Card', 'Credit Report or Prepaid Card', 'Credit Report or Prepaid Card', 'Bank Account Services', 'Mortgage/Loan', 'Bank Account Services', 'Bank Account Services', 'Bank Account Services', 'Credit Report or Prepaid Card', 'Bank Account Services', 'Mortgage/Loan', 'Credit Report or Prepaid Card', 'Credit Report or Prepaid Card', 'Credit Report or Prepaid Card', 'Bank Account Services', 'Credit Report or Prepaid Card']\n"
     ]
    }
   ],
   "source": [
    "# Vectorizo la data\n",
    "data_procesada = run_preprocessing_fn(data_analizar[\"_source.complaint_what_happened\"])\n",
    "\n",
    "# Realizo la predicci√≥n y la decodifico ()\n",
    "preds = model.predict(data_procesada)\n",
    "\n",
    "preds_list = []\n",
    "\n",
    "for pred in preds:\n",
    "    decoded_predictions = label_mapping[str(pred)]\n",
    "    preds_list.append(decoded_predictions)\n",
    "\n",
    "print(preds_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
